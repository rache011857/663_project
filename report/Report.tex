\documentclass{article} % For LaTeX2e
\usepackage{663report,times}
\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{textcomp}
\usepackage{listings}

%\documentstyle[nips14submit_09,times,art10]{article} % For LaTeX 2.09


\title{The Multiple-Try Metropolis and its Variations}


\author{
Xu Chen\\
Department of Statistical Science\\
\texttt{xu.chen2@duke.edu} \\
\And
Menglan Jiang\\
Department of Statistical Science\\
\texttt{menglan.jiang@duke.edu}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}


\maketitle

\begin{abstract}
Markov chain Monte Carlo (MCMC) has been extensively applied in many complicated computational problems to sample from an arbitrary distribution. The fundamental idea is to generate a Markov chain whose invariant distribution is the target distribution. The traditional Metropolis-Hastings algorithm (MH) based on local search may suffer from slow converging problem since the sampler may get stuck in a local mode especially for multimodal parameter spaces. Multiple-try Metropolis (MTM) was proposed to overcome this difficulty by proposing multiple trial points and then sampling based on their importance. The numerical experiments illustrate that the sampler can efficiently explore the parameter space. This project will prove the validity of MTM and implement the algorithm and its variations including Griddy-Gibbs Multiple-Try Metropolis (MTM-Gibbs) and Langevin-within-MTM on artificial data and real dataset. Comparisons are made to show the superiority of the algorithm over traditional MH algorithms.
\end{abstract}

\section{Introduction}



\section{The algorithm and its variations}
In this section, we will introduce the multiple-try metropolis as well as its variations. In the traditional M-H algorithm, we have the following settings. Firstly, a proposal function $T(x_i,x_{i+1})$ is defined, which clarifies the relationship between the $t$th trial and the $(t+1)$th candidate parameters. Next, a function used for evaluating the likelihood of a trial is defined as follows,
\begin{equation}
w(x_i,x_{i+1}) = \pi(x_i) T(x_i,x_{i+1}) \lambda (x_i,x_{i+1})
\end{equation}
where $\pi(x_i)$ is the probability distribution of $x_i$, $\lambda (x_i,x_{i+1})$ is an adjusting function to enhance the power of the algorithm, which is nonnegative and symmetric.

Two basic requirements should be satisfied.\\
1. $T(x_i,x_{i+1})  > 0$ if and only if $T(x_{i+1},x_i)  > 0$\\
2. If $T(x_i,x_{i+1})  > 0$, then $\lambda (x_i,x_{i+1}) >0$.\\
The choices of $\lambda (x_i,x_{i+1})$ vary in different situations. 


\subsection{Multiple-try Metropolis}
The MTM algorithm can be achieved through 4 steps.\\
1. Sample $k$ iid trials $x_{t+1}^{1},..., x_{t+1}^{k}$ from $T(x_i, .)$. Compute $w(x_{t+1}^{j},x_{t})$ for $j=1,...,k$.\\
2. Select $\mathbb{X} = x_{t+1}$ among the proposal set $\{x_{t+1}^{1},..., x_{t+1}^{k}\}$ with probability propotional to $\{w(x_{t+1}^{1,x_i}),...,w(x_{t+1}^{k},x_i)\}$.\\
3. Sample $x_{\star}^{1},...,x_{\star}^{k-1}$ from the distribution $T(x_{t+1},.)$, and let $x_{\star}^{k}=x_{t}$.\\
4. Accept $x_{t+1}$ with probability 
\begin{equation}
r = \text{min}\left\{ 1, \frac{w(x_{t+1}^{1},x_{t})+\cdot \cdot \cdot+w(x_{t+1}^{k},x_{t})} {w(x_{\star}^{1},x_{t+1})+\cdot \cdot \cdot+w(x_{\star}^{k},x_{t+1})} \right\}
\end{equation}
and reject it with probability $1-r$. The quantity $r$ is called generlaized M-H ratio. 
\subsection{Conjugate-Gradient Monte Carlo}




\section{Implementation}



\section{Optimization and high performance computing}



\section{Experimental results and comparisons}


\section{Conclusions}


\subsubsection*{References}


\small{
[1] Faming Liang, Chuanhai Liu \& Raymond Carroll (2011) {\it Advanced Markov chain Monte Carlo methods: learning from past samples} vol:714 John Wiley \& Sons

[2] Jun S. Liu (2001) {\it Monte Carlo Strategies in Scientific Computing} Statistics, Springer-Verlag, New York

[3] Jun S. Liu, Faming Liang \& Wing Hung Wong (2001) The Multiple-try method and local optimization in Metropolis Sampling. {\it Journal of the American Statistical Association}, 95:449, pp. 121-134

[4] Radford M. Neal (2011) MCMC using Hamiltonian dynamics. In Steve Brooks et al. (eds.) {\it Handbook of Markov chain Monte Carlo} Chapter 5, Chapman \& Hall/CRC Press
}
\end{document}
